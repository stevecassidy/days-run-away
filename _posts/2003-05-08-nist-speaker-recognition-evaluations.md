---
id: 229
title: 'NIST Speaker Recognition  Evaluations'
date: 2003-05-08T14:00:00+00:00
author: Steve Cassidy
layout: single
guid: http://www.ics.mq.edu.au/~cassidy/wordpress/?p=229
categories:
  - Uncategorized
---
Looking at the 2003 NIST SR evaluations, while we're too late to enter this year there is some useful data available, for example the [Automatically Generated word transcripts](http://www.nist.gov/speech/tests/spk/2002/extended-data/asr_trans_sid02.v0.tar.gz) for some of their training data might be interesting to look at. 

In their Rich Transcription track there is an interesting task which might be nice to attempt: meta data extraction. In the last evaluation it was just &#8220;Who Spoke When&#8221; annotation but they intend to add more target metadata in future rounds. This is quite close to some of our goals for the [Meeting Room Project](http://www.clt.mq.edu.au/Research/Projects/meeting/). 

Quote from the [NIST TREC-9 SDR page](http://www.nist.gov/speech/tests/sdr/sdr2000/sdr2000.htm): 

> The results of the TREC-9 2000 SDR evaluation presented at TREC on November 14, 2000 showed that retrieval performance for sites on their own recognizer transcripts was virtually the same as their performance on the human reference transcripts. Therefore, retrieval of excerpts from broadcast news using automatic speech recognition for transcription was deemed to be a solved problem - even with word error rates of 30%.

Gosh! 

And there's more...[Transcripts](http://www.nist.gov/speech/tests/rt/rt2002/meetingdata/) of meeting room data are available which seem to be manual. Meeting content doesn't seem too exciting ðŸ™‚ but gives some idea of dialogue structure etc. in this kind of data.
